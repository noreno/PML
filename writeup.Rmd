---
title: 'Practical Machine Learning: Prediction Assignment'
author: "Vicke Norén"
date: "May 24, 2015"
output:
  html_document:
    theme: united
    toc: yes
---

# Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, which is a part of the Practical Machine Learning course organized by Jeff Leek, PhD, Roger D. Peng, PhD, and Brian Caffo, PhD, Johns Hopkins Bloomberg School of Public Health, through Coursera, we use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information about the data is available from the following website: <http://groupware.les.inf.puc-rio.br/har>

# Cleaning data
The goal of this project is to predict the manner in which the participants did the exercises, which is the outcome variable `classe`. This variable is a factor variable with 5 levels, A-E, where each letter indicates what kind of exercise an individual has done. The participants have been asked to do 5 different weightlifting dumbbell exercises where only one exercise has been done correctly. We have two data sets that we load with:
```{r}
datasample <- read.csv("pml-training.csv")
finalTesting <- read.csv("pml-testing.csv")
```
The first one contains a large number of observations (`r nrow(datasample)` observations), while the other one only contains `r ncol(datasample)` observations:
```{r}
dim(datasample)
dim(finalTesting)
```
We see that there are `r ncol(datasample)` variables in total. A first thing to do is to look at the quality of the data:
```{r}
sum(colSums(is.na(datasample)) > 0)
sum(colSums(is.na(finalTesting)) > 0)
```
We see that there are `r sum(colSums(is.na(finalTesting)) > 0)` variables in the `finalTesting` data set with missing values that we cannot use for prediction. We therefore drop those variables from both the training and testing data sets. Looking carefully at the training data we see that many of the variables have a high percentage of missing values. They coincide with the missing variables in the testing data.

The variables we will be using to train our model are stored in `keep`:
```{r}
keep <- intersect(names(datasample), names(finalTesting)[colSums(is.na(finalTesting)) == 0])
```
We also remove variables that contain no information that can be used for prediction:
```{r}
keep <- setdiff(keep, c("X", "raw_timestamp_part_1", "raw_timestamp_part_2", 
                        "cvtd_timestamp", "new_window", "num_window", "problem_id"))
```
The training data set is reduced to:
```{r}
datasample <- datasample[, c("classe", keep)]
```

# Splitting data
Having cleaned the data we can now split our data using the `caret` and `AppliedPredictiveModeling` packages:
```{r message = FALSE}
library(caret)
library(AppliedPredictiveModeling)
```
We stick to the rule of thumb saying that 60% of the data should be used for training 40% be used for testing:
```{r}
set.seed(33733)
indTrain <- createDataPartition(datasample$classe, p = 0.6, list=FALSE)
training <- datasample[indTrain,]
testing <- datasample[-indTrain,]
dim(training)
dim(testing)
```
The function `set.seed(n)` makes sure that we draw the same random number sequence for a given `n` so that we can compare different machine learning algorithms.

# Exploratory analysis
Before training some models it is a good idea to look at the data to see whether we can find any interesting patterns. To do principal component analysis we have to reduce our data to contain only numerical variables:
```{r}
nums <- subset(training, select = sapply(training, is.numeric))
```

## Principal component analysis
Then we can do principal component analysis to find out which principal components explain most of the variability of the data:
```{r}
pca <- prcomp(nums, center = TRUE, scale = TRUE)
round(summary(pca)[[6]][3,1], digits = 3)
```
Here the number `r round(summary(pca)[[6]][3,1], digits = 3)` means that the first principal component explains `r round(100*summary(pca)[[6]][3,1], digits = 1)`% of the variability of the data. This is quite low, meaning that PCA will probably not do so well for prediction. Note that we have standardized the variables before doing the PCA. However, we have not applied the log transform because many variables have both positive and negative observations. This can be confirmed by looking at the output from `summary(training)`. With `summary(pca)` we can see statistics for all principal components.

## Correlation analysis
We can also look at correlation. First we add the `classe` and `user_name` variables as numbers:
```{r}
nums$classenum <- as.integer(training$classe)
nums$user_namenum <- as.integer(training$user_name)
```
Then we can look at the variables that have the highest correlations with each other.
```{r}
cormatrix <- cor(nums)
diag(cormatrix) <- 0
cormatrix[lower.tri(cormatrix)] <- 0
cormatrix <- as.data.frame(as.table(cormatrix))
names(cormatrix) <- c("First.Variable", "Second.Variable","Correlation")
head(cormatrix[order(abs(cormatrix$Correlation), decreasing = TRUE),], 5)
```
We see that many variables are highly correlated. We can also see which variables are most correlated with `classe`:
```{r}
corrs <- data.frame(Variable = NA, Correlation = NA)
for (i in 1:ncol(nums)) {
  if (names(nums)[i] != "classenum") {
    corrs[i,"Variable"] <- names(nums)[i]
    corrs[i,"Correlation"] <- cor(nums[,"classenum"], nums[,i])
  }
}
head(corrs[order(abs(corrs$Correlation), decreasing = TRUE),], 5)
```
The variable `pitch_forearm` has the highest correlation (0.339). No variable has a remarkably high correlation.

## Visualization
It is a good idea to visualize some of the variables to get a better feeling of how they are related to the outcome variable. Here we plot `training$roll_belt` and `pitch_forearm` against `classe`:
```{r, echo = FALSE}
library(gridExtra)
p1 <- qplot(training$roll_belt, training$classe)
p2 <- qplot(training$pitch_forearm, training$classe)
grid.arrange(p1, p2, ncol=2)
```

We also combine them together:

```{r, echo = FALSE}
qplot(training$roll_belt, training$pitch_forearm, col = training$classe)
```

There appears to be some patterns that our machine learning algorithms should be able to detect to distinguish the values of the `classe` variable.

# Model training
Feeling confident with our exploratory analysis we move on to the model training.

## k-means
First we try to fit a k-means algorithm:
```{r}
modelFit <- kmeans(subset(nums, select = -c(classenum, user_namenum)), 5)
confmatrix <- confusionMatrix(as.vector(modelFit$cluster), nums$classenum)
confmatrix$overall[1]
```
We see that k-means is not a good classifier as the accuracy is only `r round(100*confmatrix$overall[1], digits = 1)`%.

## The caret package
Next we use the built-in models in the `caret` package. For the model training we use 10-fold cross validation:
```{r}
myTrainControl <- trainControl(method = "cv", number=10)
```
I have tried many models and different ways of preprocessing the data. I predicted the `classe` variable using all the other variables from the cleaned training data. No feature selection was needed as the machine learning methods worked relatively fast on the cleaned training data. As said before, neither log transforms or PCA worked well. In the end I standardized the variables with respect to center and scale: `preProcess = c("center", "scale")`. I found out that some models were superior in terms of accuracy, especially these four:

* random forests (rf)
* quadratic discriminant analysis (qda)
* k-nearest neighbors (knn)
* generalized boosted regression modeling (gbm)

The models where trained in the following way:
```{r message = FALSE}
#modelFitQDA <- train(classe ~ ., data = trainingF, preProcess = c("center", "scale"), 
#                   trControl = myTrainControl, method = "qda")
#modelFitRF <- train(classe ~ ., data = trainingF, preProcess = c("center", "scale"), 
#                   trControl = myTrainControl, method = "rf")
#modelFitKNN <- train(classe ~ ., data = trainingF, preProcess = c("center", "scale"), 
#                   trControl = myTrainControl, method = "knn")
#modelFitGBM <- train(classe ~ ., data = trainingF, preProcess = c("center", "scale"), 
#                   trControl = myTrainControl, method = "gbm")
#save(modelFitQDA, modelFitRF, modelFitKNN, modelFitGBM, file = "modelFit.RData")
load("modelFit.RData")
```
To save some time I have commented all the actual training commands and instead loaded the trained models saved in `modelFit.RData`.

A feature of random forests is that they rank variables in terms of their importance:
```{r message = FALSE}
plot(varImp(modelFitRF), top = 10)
```

Here we can see that `roll_belt`, `pitch_forearm`, and `yaw_belt` were the three most important variables for fitting the model.

# Results in terms of accuracy
Once we have the trained the models we can look at their in-sample and out-of-sample accuracy:
```{r message = FALSE}
accInSample <- data.frame(qda = confusionMatrix(predict(modelFitQDA, newdata = training), 
                                                training$classe)$overall[[1]],
                          rf = confusionMatrix(predict(modelFitRF, newdata = training), 
                                               training$classe)$overall[[1]],
                          knn = confusionMatrix(predict(modelFitKNN, newdata = training), 
                                                training$classe)$overall[[1]],
                          gbm = confusionMatrix(predict(modelFitGBM, newdata = training), 
                                                training$classe)$overall[[1]])
accOutOfSample <- data.frame(qda = confusionMatrix(predict(modelFitQDA, newdata = testing), 
                                                   testing$classe)$overall[[1]],
                          rf = confusionMatrix(predict(modelFitRF, newdata = testing), 
                                               testing$classe)$overall[[1]],
                          knn = confusionMatrix(predict(modelFitKNN, newdata = testing), 
                                                testing$classe)$overall[[1]],
                          gbm = confusionMatrix(predict(modelFitGBM, newdata = testing), 
                                                testing$classe)$overall[[1]])
round(accInSample, digits = 3)
round(accOutOfSample, digits = 3)
```
As we can see, these four models perform very well, especially random forests, which are able to classify all classes correctly in the case of using the in-sample data. Naturally, the out-of-sample accuracy is lower than the in-sample accuracy, but the difference is very small. From the accuracy measures we know that the models are performing extremely well and are not overfitting the data, therefore we should expect the models to do well in the final prediction. When comparing the models we see that random forests perform the best. Consequently, this was the method I used for the real prediction of `classe` using `finalTesting`:
```{r}
predictedClasse <- predict(modelFitRF, newdata = finalTesting)
```
It is sound to compare the results of the predictions of the models and look at their similarities and differences to see whether there are any deviations across models. Using this approach I was able to score 20 out of 20 on the prediction assignment.


